# Feature-Scaling

Feature scaling is a data preprocessing technique used to standardize the range of independent variables (features) in a dataset. It ensures that numerical features contribute proportionally to model training, preventing features with larger magnitudes from dominating those with smaller ones.

Why Feature Scaling Is Important

Many machine learning algorithms rely on distance calculations, gradient optimization, or variance assumptions. When features are on different scales, these algorithms can become biased, unstable, or slow to converge.

Feature scaling helps to:

Improve model convergence speed

Ensure fair contribution of all features

Enhance numerical stability
